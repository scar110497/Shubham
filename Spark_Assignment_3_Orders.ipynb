{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4azUfa/3lNAbLh84ahBFo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scar110497/Shubham/blob/main/Spark_Assignment_3_Orders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enElTmm5-zsb"
      },
      "outputs": [],
      "source": [
        "#install Apache Spark 3.0.1 with Hadoop 2.7 from here.\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "\n",
        "# Now, we just need to unzip that folder.\n",
        "!tar -xvzf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, DoubleType\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SparkDemoApp\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(type(spark))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem Statement: What is the daily product revenue for CLOSED or COMPLETE orders\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "user_df = spark.read.csv(path='/content/order', header=False)\n",
        "user_df.show()\n",
        "\n",
        "status = user_df[\"_c3\"].isin([\"CLOSED\", \"COMPLETE\"])\n",
        "user_df_filtered = user_df.filter(status)\n",
        "\n",
        "revenue = user_df_filtered.groupBy(date_format(\"_c1\", \"yyyy-MM-dd\").alias(\"date\")).agg(sum(\"_c2\").alias(\"daily_product_revenue\"))\n",
        "\n",
        "revenue.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLRtSkfkAN9H",
        "outputId": "9a245a14-57f3-42cd-df95-bd9b422c72cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-----+---------------+\n",
            "|_c0|                 _c1|  _c2|            _c3|\n",
            "+---+--------------------+-----+---------------+\n",
            "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
            "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
            "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
            "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
            "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
            "|  6|2013-07-25 00:00:...| 7130|       COMPLETE|\n",
            "|  7|2013-07-25 00:00:...| 4530|       COMPLETE|\n",
            "|  8|2013-07-25 00:00:...| 2911|     PROCESSING|\n",
            "|  9|2013-07-25 00:00:...| 5657|PENDING_PAYMENT|\n",
            "| 10|2013-07-25 00:00:...| 5648|PENDING_PAYMENT|\n",
            "| 11|2013-07-25 00:00:...|  918| PAYMENT_REVIEW|\n",
            "| 12|2013-07-25 00:00:...| 1837|         CLOSED|\n",
            "| 13|2013-07-25 00:00:...| 9149|PENDING_PAYMENT|\n",
            "| 14|2013-07-25 00:00:...| 9842|     PROCESSING|\n",
            "| 15|2013-07-25 00:00:...| 2568|       COMPLETE|\n",
            "| 16|2013-07-25 00:00:...| 7276|PENDING_PAYMENT|\n",
            "| 17|2013-07-25 00:00:...| 2667|       COMPLETE|\n",
            "| 18|2013-07-25 00:00:...| 1205|         CLOSED|\n",
            "| 19|2013-07-25 00:00:...| 9488|PENDING_PAYMENT|\n",
            "| 20|2013-07-25 00:00:...| 9198|     PROCESSING|\n",
            "+---+--------------------+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+---------------------+\n",
            "|date      |daily_product_revenue|\n",
            "+----------+---------------------+\n",
            "|2014-02-16|757942.0             |\n",
            "|2014-02-22|365090.0             |\n",
            "|2014-05-27|240185.0             |\n",
            "|2014-07-14|453331.0             |\n",
            "|2013-08-20|535789.0             |\n",
            "|2013-12-28|662309.0             |\n",
            "|2013-11-08|450352.0             |\n",
            "|2014-03-17|259678.0             |\n",
            "|2014-03-20|517343.0             |\n",
            "|2013-09-27|815868.0             |\n",
            "|2014-07-24|536326.0             |\n",
            "|2014-02-08|445823.0             |\n",
            "|2014-05-23|445204.0             |\n",
            "|2013-11-05|799736.0             |\n",
            "|2013-10-05|552018.0             |\n",
            "|2013-10-29|312465.0             |\n",
            "|2014-05-01|379722.0             |\n",
            "|2014-06-02|582818.0             |\n",
            "|2014-04-02|616979.0             |\n",
            "|2014-07-11|334643.0             |\n",
            "+----------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the required data in to DF like categories, customer,departments,order_items,orders and products\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Order_data_Analysis\").getOrCreate()\n",
        "\n",
        "categories = spark.read.csv(path = \"/content/category\",header=False)\n",
        "customer = spark.read.csv(path = \"/content/Customer\",header=False)\n",
        "departments = spark.read.csv(path = \"/content/dept\",header=False)\n",
        "order_items = spark.read.csv(path = \"/content/order_item\",header=False)\n",
        "orders = spark.read.csv(path = \"/content/order\",header=False)\n",
        "products = spark.read.csv(path = \"/content/product\",header=False)\n",
        "categories.show(n=5)\n",
        "customer.show(n=5)\n",
        "departments.show(n=5)\n",
        "order_items.show(n=5)\n",
        "orders.show(n=5)\n",
        "products.show(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58wD5vVFBnxz",
        "outputId": "a9d99c86-3278-4ae1-a08c-642caa5f5876"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------------------+\n",
            "|_c0|_c1|                _c2|\n",
            "+---+---+-------------------+\n",
            "|  1|  2|           Football|\n",
            "|  2|  2|             Soccer|\n",
            "|  3|  2|Baseball & Softball|\n",
            "|  4|  2|         Basketball|\n",
            "|  5|  2|           Lacrosse|\n",
            "+---+---+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+-------+---------+---------+---------+--------------------+-----------+---+-----+\n",
            "|_c0|    _c1|      _c2|      _c3|      _c4|                 _c5|        _c6|_c7|  _c8|\n",
            "+---+-------+---------+---------+---------+--------------------+-----------+---+-----+\n",
            "|  1|Richard|Hernandez|XXXXXXXXX|XXXXXXXXX|  6303 Heather Plaza|Brownsville| TX|78521|\n",
            "|  2|   Mary|  Barrett|XXXXXXXXX|XXXXXXXXX|9526 Noble Embers...|  Littleton| CO|80126|\n",
            "|  3|    Ann|    Smith|XXXXXXXXX|XXXXXXXXX|3422 Blue Pioneer...|     Caguas| PR|00725|\n",
            "|  4|   Mary|    Jones|XXXXXXXXX|XXXXXXXXX|  8324 Little Common| San Marcos| CA|92069|\n",
            "|  5| Robert|   Hudson|XXXXXXXXX|XXXXXXXXX|10 Crystal River ...|     Caguas| PR|00725|\n",
            "+---+-------+---------+---------+---------+--------------------+-----------+---+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+--------+\n",
            "|_c0|     _c1|\n",
            "+---+--------+\n",
            "|  2| Fitness|\n",
            "|  3|Footwear|\n",
            "|  4| Apparel|\n",
            "|  5|    Golf|\n",
            "|  6|Outdoors|\n",
            "+---+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+---+----+---+------+------+\n",
            "|_c0|_c1| _c2|_c3|   _c4|   _c5|\n",
            "+---+---+----+---+------+------+\n",
            "|  1|  1| 957|  1|299.98|299.98|\n",
            "|  2|  2|1073|  1|199.99|199.99|\n",
            "|  3|  2| 502|  5| 250.0|  50.0|\n",
            "|  4|  2| 403|  1|129.99|129.99|\n",
            "|  5|  4| 897|  2| 49.98| 24.99|\n",
            "+---+---+----+---+------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+--------------------+-----+---------------+\n",
            "|_c0|                 _c1|  _c2|            _c3|\n",
            "+---+--------------------+-----+---------------+\n",
            "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
            "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
            "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
            "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
            "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
            "+---+--------------------+-----+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+---+--------------------+----+------+--------------------+\n",
            "|_c0|_c1|                 _c2| _c3|   _c4|                 _c5|\n",
            "+---+---+--------------------+----+------+--------------------+\n",
            "|  1|  2|Quest Q64 10 FT. ...|null| 59.98|http://images.acm...|\n",
            "|  2|  2|Under Armour Men'...|null|129.99|http://images.acm...|\n",
            "|  3|  2|Under Armour Men'...|null| 89.99|http://images.acm...|\n",
            "|  4|  2|Under Armour Men'...|null| 89.99|http://images.acm...|\n",
            "|  5|  2|Riddell Youth Rev...|null|199.99|http://images.acm...|\n",
            "+---+---+--------------------+----+------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count for each order status\n",
        "\n",
        "order_sc = (orders.groupBy(\"_c3\").agg(count(\"_c0\").alias(\"Count_of_order_status\")).withColumnRenamed(\"_c3\", \"order_status\"))\n",
        "\n",
        "order_sc.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSEmJ4BRCYwN",
        "outputId": "fd7d50c4-e005-467a-dfe3-9c0d52c53bf1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------------+\n",
            "|order_status   |Count_of_order_status|\n",
            "+---------------+---------------------+\n",
            "|PENDING_PAYMENT|15030                |\n",
            "|COMPLETE       |22899                |\n",
            "|ON_HOLD        |3798                 |\n",
            "|PAYMENT_REVIEW |729                  |\n",
            "|PROCESSING     |8275                 |\n",
            "|CLOSED         |7556                 |\n",
            "|SUSPECTED_FRAUD|1558                 |\n",
            "|PENDING        |7610                 |\n",
            "|CANCELED       |1428                 |\n",
            "+---------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only COMPLETE or CLOSED orders\n",
        "\n",
        "user_df_filtered = user_df.filter(user_df[\"_c3\"].isin([\"COMPLETE\", \"CLOSED\"]))\n",
        "user_df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxNBXgI9Cobq",
        "outputId": "05b74416-2b40-4efb-8a11-78cada0c4123"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-----+--------+\n",
            "|_c0|                 _c1|  _c2|     _c3|\n",
            "+---+--------------------+-----+--------+\n",
            "|  1|2013-07-25 00:00:...|11599|  CLOSED|\n",
            "|  3|2013-07-25 00:00:...|12111|COMPLETE|\n",
            "|  4|2013-07-25 00:00:...| 8827|  CLOSED|\n",
            "|  5|2013-07-25 00:00:...|11318|COMPLETE|\n",
            "|  6|2013-07-25 00:00:...| 7130|COMPLETE|\n",
            "|  7|2013-07-25 00:00:...| 4530|COMPLETE|\n",
            "| 12|2013-07-25 00:00:...| 1837|  CLOSED|\n",
            "| 15|2013-07-25 00:00:...| 2568|COMPLETE|\n",
            "| 17|2013-07-25 00:00:...| 2667|COMPLETE|\n",
            "| 18|2013-07-25 00:00:...| 1205|  CLOSED|\n",
            "| 22|2013-07-25 00:00:...|  333|COMPLETE|\n",
            "| 24|2013-07-25 00:00:...|11441|  CLOSED|\n",
            "| 25|2013-07-25 00:00:...| 9503|  CLOSED|\n",
            "| 26|2013-07-25 00:00:...| 7562|COMPLETE|\n",
            "| 28|2013-07-25 00:00:...|  656|COMPLETE|\n",
            "| 32|2013-07-25 00:00:...| 3960|COMPLETE|\n",
            "| 35|2013-07-25 00:00:...| 4840|COMPLETE|\n",
            "| 37|2013-07-25 00:00:...| 5863|  CLOSED|\n",
            "| 45|2013-07-25 00:00:...| 2636|COMPLETE|\n",
            "| 51|2013-07-25 00:00:...|12271|  CLOSED|\n",
            "+---+--------------------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the products , order_items and orders tables and calculate daily product revenue\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SparkDemo\").getOrCreate()\n",
        "\n",
        "combine_df = (order_items.alias(\"oi\").join(products.alias(\"p\"), col(\"oi._c2\") == col(\"p._c0\"), \"inner\").join(orders.alias(\"o\"), col(\"oi._c1\") == col(\"o._c0\"), \"inner\"))\n",
        "\n",
        "dp_revenue = (combine_df.withColumn(\"order_date\", to_date(\"o._c1\")).groupBy(\"order_date\", \"p._c4\") .agg(sum(\"oi._c4\").alias(\"daily_revenue\")).orderBy(\"order_date\"))\n",
        "dp_revenue.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i00yeuDlC30H",
        "outputId": "f990320d-ca2a-465f-dc2b-d4fc9fc1f496"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+------------------+\n",
            "|order_date|_c4   |daily_revenue     |\n",
            "+----------+------+------------------+\n",
            "|2013-07-25|149.99|299.98            |\n",
            "|2013-07-25|199.99|7399.629999999995 |\n",
            "|2013-07-25|599.99|599.99            |\n",
            "|2013-07-25|51.99 |207.96            |\n",
            "|2013-07-25|79.99 |559.93            |\n",
            "|2013-07-25|31.99 |543.8299999999999 |\n",
            "|2013-07-25|15.99 |159.9             |\n",
            "|2013-07-25|9.99  |19.98             |\n",
            "|2013-07-25|109.99|219.98            |\n",
            "|2013-07-25|50.0  |5100.0            |\n",
            "|2013-07-25|108.0 |216.0             |\n",
            "|2013-07-25|21.99 |21.99             |\n",
            "|2013-07-25|19.99 |79.96             |\n",
            "|2013-07-25|299.98|9599.359999999993 |\n",
            "|2013-07-25|99.0  |297.0             |\n",
            "|2013-07-25|99.99 |8499.149999999998 |\n",
            "|2013-07-25|129.99|5589.569999999994 |\n",
            "|2013-07-25|25.0  |100.0             |\n",
            "|2013-07-25|34.99 |69.98             |\n",
            "|2013-07-25|399.98|10799.459999999994|\n",
            "+----------+------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data in to the table Daily product revenue in Hive\n",
        "\n",
        "order = \"dr_shubham.dp_revenue\"\n",
        "\n",
        "dp_revenue.write.mode(\"overwrite\").saveAsTable(order)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Vt4tfrWJDbXS",
        "outputId": "cfff1927-e757-4065-cfe2-185ea64c326b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fb704f768e86>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dr_shubham.dp_revenue\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdp_revenue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Database 'dr_shubham' not found;"
          ]
        }
      ]
    }
  ]
}